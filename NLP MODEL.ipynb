{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377acb2e-db76-492c-8b01-83e6710881a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.0/44.0 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nagesh\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.0 MB 2.0 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.1/10.0 MB 2.1 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/10.0 MB 1.6 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/10.0 MB 1.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/10.0 MB 1.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/10.0 MB 1.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/10.0 MB 1.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.5/10.0 MB 1.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/10.0 MB 1.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/10.0 MB 1.4 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.7/10.0 MB 1.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.8/10.0 MB 1.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/10.0 MB 1.5 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/10.0 MB 1.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/10.0 MB 1.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.2/10.0 MB 1.6 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 1.7 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.4/10.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/10.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/10.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/10.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/10.0 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/10.0 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/10.0 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/10.0 MB 1.7 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.9/10.0 MB 1.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.9/10.0 MB 1.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.0/10.0 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.0 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.0 MB 1.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.1/10.0 MB 1.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.4 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.2/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.3/10.0 MB 1.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.4/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.5/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.5/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.5/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 2.5/10.0 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.5/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 2.8/10.0 MB 1.0 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.0/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.0/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.0/10.0 MB 1.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.0/10.0 MB 989.9 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.0/10.0 MB 989.9 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.1/10.0 MB 971.7 kB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.1/10.0 MB 973.4 kB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.2/10.0 MB 969.1 kB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.2/10.0 MB 973.9 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 973.9 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.3/10.0 MB 980.0 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 3.4/10.0 MB 787.0 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.4/10.0 MB 786.0 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.5/10.0 MB 786.4 kB/s eta 0:00:09\n",
      "   ------------- -------------------------- 3.5/10.0 MB 777.8 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.5/10.0 MB 783.6 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.6/10.0 MB 791.5 kB/s eta 0:00:09\n",
      "   -------------- ------------------------- 3.6/10.0 MB 796.8 kB/s eta 0:00:08\n",
      "   -------------- ------------------------- 3.7/10.0 MB 797.1 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.8/10.0 MB 806.6 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.8/10.0 MB 806.6 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.8/10.0 MB 797.3 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.9/10.0 MB 804.8 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.9/10.0 MB 808.1 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.9/10.0 MB 799.9 kB/s eta 0:00:08\n",
      "   --------------- ------------------------ 3.9/10.0 MB 796.5 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.0/10.0 MB 802.8 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 806.5 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 814.7 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 814.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.2/10.0 MB 817.7 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 4.3/10.0 MB 821.3 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 4.4/10.0 MB 830.5 kB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 4.4/10.0 MB 835.1 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 4.5/10.0 MB 840.8 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 4.6/10.0 MB 845.4 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 4.6/10.0 MB 850.7 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 4.7/10.0 MB 855.2 kB/s eta 0:00:07\n",
      "   ------------------ --------------------- 4.7/10.0 MB 855.2 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 4.7/10.0 MB 848.1 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 4.8/10.0 MB 847.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 4.8/10.0 MB 850.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 4.9/10.0 MB 846.5 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 5.0/10.0 MB 860.2 kB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.0/10.0 MB 860.2 kB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.0/10.0 MB 860.2 kB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.0/10.0 MB 860.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.1/10.0 MB 850.3 kB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.1/10.0 MB 849.9 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.3/10.0 MB 870.4 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.3/10.0 MB 871.6 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.3/10.0 MB 870.4 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.3/10.0 MB 863.3 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.4/10.0 MB 871.0 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.4/10.0 MB 865.7 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.4/10.0 MB 865.7 kB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.5/10.0 MB 859.9 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 5.5/10.0 MB 858.4 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 863.3 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 864.9 kB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 5.6/10.0 MB 860.2 kB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 5.8/10.0 MB 869.2 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 5.9/10.0 MB 892.3 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.0/10.0 MB 893.7 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.0/10.0 MB 892.0 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.1/10.0 MB 892.5 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.1/10.0 MB 891.7 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.1/10.0 MB 889.2 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.2/10.0 MB 888.6 kB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.2/10.0 MB 891.9 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.3/10.0 MB 893.4 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.3/10.0 MB 897.6 kB/s eta 0:00:05\n",
      "   ------------------------- -------------- 6.4/10.0 MB 895.9 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 6.5/10.0 MB 907.2 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.6/10.0 MB 906.4 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.6/10.0 MB 915.2 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 6.7/10.0 MB 915.9 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 6.7/10.0 MB 921.5 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 6.7/10.0 MB 921.5 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 6.8/10.0 MB 916.4 kB/s eta 0:00:04\n",
      "   --------------------------- ------------ 6.9/10.0 MB 924.1 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.0/10.0 MB 932.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.0/10.0 MB 927.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 923.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 932.9 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 935.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 935.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 935.9 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 935.9 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.3/10.0 MB 922.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.4/10.0 MB 923.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 7.4/10.0 MB 929.9 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.5/10.0 MB 933.9 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.5/10.0 MB 931.1 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 7.6/10.0 MB 940.2 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.7/10.0 MB 944.3 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.8/10.0 MB 949.4 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 7.9/10.0 MB 952.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/10.0 MB 956.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/10.0 MB 952.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/10.0 MB 956.9 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.1/10.0 MB 963.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.2/10.0 MB 965.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.3/10.0 MB 966.4 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.3/10.0 MB 968.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.4/10.0 MB 968.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 8.5/10.0 MB 972.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.5/10.0 MB 975.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.6/10.0 MB 979.6 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 979.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/10.0 MB 981.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/10.0 MB 982.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.8/10.0 MB 983.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/10.0 MB 984.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.0/10.0 MB 994.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/10.0 MB 994.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/10.0 MB 995.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 997.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/10.0 MB 999.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/10.0 MB 999.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/10.0 MB 998.4 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/10.0 MB 998.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 1.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "   ---------------------------------------- 0.0/464.1 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 71.7/464.1 kB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 256.0/464.1 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 348.2/464.1 kB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  460.8/464.1 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 464.1/464.1 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/303.8 kB 3.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 153.6/303.8 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 204.8/303.8 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  297.0/303.8 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 303.8/303.8 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.4 MB 4.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.4 MB 4.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.4 MB 3.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.4 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.9/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.0/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.2/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.3/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.3/2.4 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.28.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86b9147-7d2a-435a-baed-dcbcabe8af5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 660.6 kB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/1.7 MB 544.7 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/1.7 MB 573.4 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/1.7 MB 547.6 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/1.7 MB 481.4 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 618.3 kB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 593.2 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.7 MB 656.0 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.3/1.7 MB 655.2 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.3/1.7 MB 655.5 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.4/1.7 MB 735.7 kB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 819.9 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.5/1.7 MB 857.5 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.6/1.7 MB 921.0 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.7/1.7 MB 975.2 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 990.5 kB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 990.5 kB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 908.0 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 908.0 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.7 MB 863.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.7 MB 853.3 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.9/1.7 MB 857.5 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 860.7 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.0/1.7 MB 858.7 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 865.0 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.1/1.7 MB 885.3 kB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.1/1.7 MB 904.5 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 923.7 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 909.9 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.7 MB 950.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 962.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.7 MB 975.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.7 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.7.0\n",
      "    Uninstalling keras-3.7.0:\n",
      "      Successfully uninstalled keras-3.7.0\n",
      "Successfully installed keras-2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.18.0 requires keras>=3.5.0, but you have keras 2.15.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.15.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b9592-b848-4b95-acf0-5b260250e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a55dc15-e5c5-4916-af03-5f160c71b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a21a546948749dd84c983247b06d692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763a6cc95e6a445e81d7bd9b83650624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4f6b4a829f458fa5a30ae1bbdf9a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9992502331733704}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "clf = pipeline('sentiment-analysis')\n",
    "# clf(\"I appreciate your like and comments\")\n",
    "# clf(\"I don't like when my students don't like my videos\")\n",
    "clf(\"sky is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "920f310e-b4ba-4e87-9241-d74e2c85bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to FacebookAI/roberta-large-mnli and revision 2a8f12d (https://huggingface.co/FacebookAI/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9562344551086426, 0.026972174644470215, 0.01679333485662937]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db411b8c-d4d3-45df-9f30-18d292ca482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ef722e48fc43a8bd94d7f0c6f140be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NAGESH\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6796c8fd554adcb48d7d78d5de5e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832f83a8529544ebb7db602eed43318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b58b4e4371041c5ada6cd75e1eb780a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147bb0d0e295455da1329568855e88a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae27395caf24bd1abf9e9a3103d75cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course we will teach you some basic knowledge and you will be able to navigate through the common pitfalls and how you can improve on any problem you encounter. Then we will share about how you will be able to build up your skill level in order'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "gen = pipeline('text-generation')\n",
    "gen(\"In this course we will teach you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f29b262-9ca7-48b8-8d2d-2bfe7274b3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9ae2b83ddc400f94ec884d7ed73674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NAGESH\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f79625a2c36408ab34f018e0e18ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f5db2b31aa43b3a382823c3fc3be50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df156bdca56c4b90bafd40946cd2e040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fd02108d4a4dd69f2326efa1d274c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d0b62664d44a369fdd6973a2668459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'in this course we will teach you how to use the tools you are used to learning in a safe environment.'},\n",
       " {'generated_text': 'in this course we will teach you how to use and to control. It‰\\n\\nThere are many common reasons why I don’n'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "\"in this course we will teach you\",\n",
    "max_length =30,\n",
    "num_return_sequences=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db85203f-6274-411d-9f80-85a83bd9aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13de30de0d9d46578d7f1322cdde7b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619695842266083,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052704572677612,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'},\n",
       " {'score': 0.0330178402364254,\n",
       "  'token': 27930,\n",
       "  'token_str': ' predictive',\n",
       "  'sequence': 'This course will teach you all about predictive models.'},\n",
       " {'score': 0.031941190361976624,\n",
       "  'token': 745,\n",
       "  'token_str': ' building',\n",
       "  'sequence': 'This course will teach you all about building models.'},\n",
       " {'score': 0.02452266775071621,\n",
       "  'token': 3034,\n",
       "  'token_str': ' computer',\n",
       "  'sequence': 'This course will teach you all about computer models.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7996b7f7-ca0a-4a20-a126-4b61756d5ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d572ebfe784b36a482b2da863c2c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NAGESH\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f382cade251465fa7c960c03eea8856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/dbmdz/bert-large-cased-finetuned-conll03-english/9a90b161380a5549418764749cabe9257dce2df7fa58bcec648289f00f982ebb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739898004&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTg5ODAwNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYm1kei9iZXJ0LWxhcmdlLWNhc2VkLWZpbmV0dW5lZC1jb25sbDAzLWVuZ2xpc2gvOWE5MGIxNjEzODBhNTU0OTQxODc2NDc0OWNhYmU5MjU3ZGNlMmRmN2ZhNThiY2VjNjQ4Mjg5ZjAwZjk4MmViYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=on0E3u1a4VArR74oCWAiAxa4G2eEdeGYJ5wyqe%7EKY2pxsDte2d-k8FVbiTFWgqaBMG%7ECejf-Mmq1ityGIlnQ7KWe%7EiGRYd7j-anGWoB3%7EFnA8pWwJ3T1Gr4%7E%7EzhL0MdIYA4v-tm2Wdnt6Y90ws2l%7EUiUd0hNFGJj4fiIIZ5XYvFiAqBbcPB4ERBenJ-sSaPDgHpQdVCX1%7E0LJHL72XJW2BIsy-wEhKDHpTeKiOWLnLzQOq0Lf9rTug2O5Y09Ynf0xTgzjxeQL3TlM3sUUscUBte0nG%7EJKwt%7EfWCiS%7EhmOd0ATEp4V%7EUa3i%7E5IAqzuZiNUU2OHJ%7Eh8sUmjnLOBRIwUg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b136b8e6f0b4398af6234fd25f12cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   4%|3         | 52.4M/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/dbmdz/bert-large-cased-finetuned-conll03-english/9a90b161380a5549418764749cabe9257dce2df7fa58bcec648289f00f982ebb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739898004&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTg5ODAwNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9kYm1kei9iZXJ0LWxhcmdlLWNhc2VkLWZpbmV0dW5lZC1jb25sbDAzLWVuZ2xpc2gvOWE5MGIxNjEzODBhNTU0OTQxODc2NDc0OWNhYmU5MjU3ZGNlMmRmN2ZhNThiY2VjNjQ4Mjg5ZjAwZjk4MmViYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=on0E3u1a4VArR74oCWAiAxa4G2eEdeGYJ5wyqe%7EKY2pxsDte2d-k8FVbiTFWgqaBMG%7ECejf-Mmq1ityGIlnQ7KWe%7EiGRYd7j-anGWoB3%7EFnA8pWwJ3T1Gr4%7E%7EzhL0MdIYA4v-tm2Wdnt6Y90ws2l%7EUiUd0hNFGJj4fiIIZ5XYvFiAqBbcPB4ERBenJ-sSaPDgHpQdVCX1%7E0LJHL72XJW2BIsy-wEhKDHpTeKiOWLnLzQOq0Lf9rTug2O5Y09Ynf0xTgzjxeQL3TlM3sUUscUBte0nG%7EJKwt%7EfWCiS%7EhmOd0ATEp4V%7EUa3i%7E5IAqzuZiNUU2OHJ%7Eh8sUmjnLOBRIwUg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac4835cc9494d1bbdf469752cd2ca58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  12%|#1        | 157M/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddb24c081ef42a4bcaeb6eac707b918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f918ff15d884d339bf5c1f37182872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n",
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d481dd20-ec0a-4ba4-bd13-4f038eae5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5409618b77bf491b8c2867341d44f99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NAGESH\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce41219a5354ea4849d2d6dc50f9211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10293fc0f54a40dfae20ab51f892f9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f390ebe13942689f2fb6e119c4879c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fd00051ddf4ce99d2e97820ef8efb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.2410239726305008, 'start': 14, 'end': 18, 'answer': 'nila'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Asman ka rang kia he?\",\n",
    "    context=\"Asman ka rang nila he. Asaman bahut uncha he ju humy nila dikhai deta he\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5205dcf-20c8-4d9f-847b-4b53790450f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f252536d0c2411a845973ad62f3b16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NAGESH\\.cache\\huggingface\\hub\\models--google-t5--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2537daf726514d76b961cad1526adfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498df41ddb8b40dfb9ac4a17687853a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d0a1e7cd3a4c55a88aeefa0464c826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc626f4ff114750a750bb62cbee0b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff4150d-b9db-4b01-b2e6-0754f62c8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NAGESH\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "kannada_text = \"ಈ ಪಾಠವನ್ನು Hugging Face ಉತ್ಪಾದಿಸಿದೆ.\"\n",
    "translation = translator(kannada_text)\n",
    "print(translation[0]['translation_text'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec112c20-5a53-4e1d-9032-592aac9f6047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
